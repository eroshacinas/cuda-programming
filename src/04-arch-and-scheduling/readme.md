# ğŸš€ Chapter 04 - GPU Architecture and Scheduling

---

## ğŸ“ GPU Architecture Overview

**GPU architecture is organized into an array of highly threaded Streaming Multiprocessors (SMs)**

Each **SM (Streaming Multiprocessor)** contains:
- ğŸ”¹ **Streaming Processors** / **CUDA Cores** (or simply "cores")
- ğŸ”¹ **Control Unit**
- ğŸ”¹ **On-chip Memory** (distinct from global memory/VRAM)

![GPU Architecture](images/gpu-arch.png)

---

## ğŸ§© Block Assignment

### Key Concepts

- âœ… **Multiple blocks** are likely to be simultaneously assigned to the same SM
- âš ï¸ Blocks require hardware resources, so only a **limited number** can be simultaneously assigned to any given SM
- ğŸ“Š There's a limit on the total number of blocks that can simultaneously execute on a CUDA device

### Assignment Guarantees

Assignment of threads to SMs occurs on a **block-by-block basis**, guaranteeing:

1. All threads in the same block are **scheduled simultaneously**
2. All threads in the same block **execute on the same SM**

> ğŸ’¡ This enables threads within a block to interact in ways that threads across different blocks cannot

![Block Assignment](images/block-assignment.png)

---

## ğŸ”„ Synchronization and Transparent Scalability

### Barrier Synchronization

CUDA allows threads in the same block to coordinate activities using:

```c
__syncthreads()
```

> âš ï¸ **Critical Rule**: `__syncthreads()` must be executed by **ALL** threads in a block

![Barrier Synchronization](images/barrier-sync.png)

### âœ… Correct Usage Rules

If `__syncthreads()` is placed within an `if` statement:
- Either **ALL** threads in a block execute the path that includes `__syncthreads()`
- OR **NONE** of them do

### âŒ Incorrect Usage Example

```c
void incorrect_barrier_example(int n) {
    // ...existing code...
    if (threadIdx.x % 2 == 0) {
        // ...existing code...
        __syncthreads();  // âŒ WRONG!
    } else {
        // ...existing code...
        __syncthreads();  // âŒ WRONG!
    }
}
```

> âš ï¸ **Why this is wrong**: The code violates the rule that all threads must execute `__syncthreads()` at the same program point. This results in **undefined behavior**.

**Consequences of incorrect usage:**
- ğŸ’¥ Incorrect results
- ğŸ”’ Deadlock

---

### ğŸ¯ Transparent Scalability

The trade-off of **not allowing** barrier synchronization between different blocks enables **transparent scalability**.

#### âŒ If blocks could synchronize with each other:
- Runtime would need to schedule all blocks requiring synchronization at the same time
- Would require enormous resources
- Would limit how the GPU could execute your code

#### âœ… By preventing inter-block synchronization:

Blocks become completely **independent execution units**, allowing the runtime to:

1. ğŸ”€ Execute blocks in **any order** (e.g., Block 0 â†’ Block 1, or Block 5 â†’ Block 2)
2. âš¡ Execute **any number** of blocks simultaneously based on available resources
3. ğŸŒ Run the same kernel on both **low-end and high-end GPUs**

**Visual Example:**

| Low-Cost GPU | High-End GPU |
|-------------|--------------|
| 2 blocks execute simultaneously | 4 blocks execute simultaneously |
| Limited execution resources | Greater available resources |

![Barrier Scalability](images/barrier-sync-scalability.png)

> ğŸ’¡ **Transparent scalability** = Execute the same application program on different hardware with **zero code changes**

---

## ğŸŒŠ Warps and SIMD Hardware

### Understanding Warps

> ğŸ“Œ Conceptually, one should assume that threads in a block can execute in **any order** with respect to each other.

#### Key Definitions

- ğŸ“¦ **Warp**: A 32-thread unit for scheduling in SMs
- ğŸ”¢ **Warp Size**: Fixed at **32 threads** (implementation-specific, may vary in future GPUs)
- ğŸ“‹ **Thread Organization**: Consecutive `threadIdx` values (0-31 â†’ first warp, 32-63 â†’ second warp, etc.)

### Warp Calculation Examples

```
Block with 256 threads:
  â†’ 256/32 = 8 warps per block
  â†’ With 3 blocks in SM: 8 Ã— 3 = 24 warps total

Block with 48 threads:
  â†’ 2 warps (second warp padded with 16 inactive threads)
```

> âš ï¸ For blocks whose size is **not a multiple of 32**, the last warp will be padded with inactive threads to fill up the 32 thread positions.

![Warp-Partitioned Blocks](images/warps.png)

### Multi-Dimensional Thread Blocks

For blocks with multiple dimensions of threads:
- Dimensions are projected into a **linearized row-major layout**
- Then partitioned into warps

![Warp-Partitioned Blocks](images/2d-warp-linearized.png)

![SM for SIMD Execution](images/sm-architecture.png)

---

## ğŸ”€ Control Divergence

### What is Control Divergence?

Threads in the same warp that follow **different execution paths** exhibit **control divergence**.

### How It Works

For an `if-else` construct:
1. If some threads follow the `if`-path and others follow the `else`-path
2. Hardware takes **two passes**:
   - ğŸ”¹ Pass 1: Execute threads following the `if`-path
   - ğŸ”¹ Pass 2: Execute threads following the `else`-path
3. During each pass, threads following the other path are **inactive**

![Threads Diverging on If-Else](images/threads-conditional-diverge.png)

---

## âš¡ Warp Scheduling and Latency Tolerance

### The Challenge

- SMs have **limited execution units** to execute only a subset of assigned threads at any point in time
- Recent designs: Each SM can execute instructions for a **small number of warps** at any given point in time

### The Solution: Latency Hiding

> ğŸ’¡ Assigning many more warps to an SM than it can execute at once is how GPUs tolerate **long-latency operations** (e.g., global memory accesses).

**Analogy**: The SM schedules many more warps than it can execute at once so that when one warp hits a **'red light'** (waiting for data from VRAM), it can switch to a **'ready' warp** in **zero clock cycles**, ensuring the math units never sit idle.

### ğŸ”§ Zero-Overhead Context Switching

The GPU achieves **zero-cycle switching** through a "brute force" hardware strategy:

| ğŸ’» CPU | ğŸ® GPU |
|--------|--------|
| One set of registers | Massive Register File |
| Must save/restore state to RAM | All warp states stored on-chip simultaneously |
| Context switch overhead | Zero-overhead switching |

**How it works:**
- When a Block is assigned to an SM, the hardware **carves out a permanent slice of registers** for those threads
- They stay there until the Block is **completely finished**

### ğŸ“Š Key Constraints

```
Block Limits:
  âœ“ Max 1,024 threads per block (regardless of x, y, z dimensions)
  âœ“ Fixed warp size of 32 threads

Register File:
  âœ“ 65,536 (64K) 32-bit registers per SM
  âœ“ SM throws error if it can't fit all states
```

> ğŸ”¬ **Why not increase thread limits?** Doubling threads per block increases hardware complexity:
> - `__syncthreads()` for 1,024 threads is manageable
> - For 10,000 threads would require massive "wait logic" circuitry
> - That space is better used for math units

**Latency Tolerance Principle:**
> ğŸ¯ For effective latency tolerance, an SM should have **many more threads assigned** than can be simultaneously executed, maximizing the chance of finding a ready warp at any point in time.

---

## ğŸ“Š Resource Partitioning and Occupancy

### Occupancy Definition

```
Occupancy = (Number of warps assigned to SM) / (Maximum warps SM supports)
```

### ğŸ› ï¸ Execution Resources

An SM's resources are dynamically partitioned across threads:

1. ğŸ“ **Registers**
2. ğŸ’¾ **Shared Memory**
3. ğŸ« **Thread Block Slots**
4. ğŸ‘¥ **Thread Slots**

### Example: Ampere A100 GPU

```
Hardware Limits:
  â€¢ Max 32 blocks per SM
  â€¢ Max 64 warps (2,048 threads) per SM
  â€¢ Max 1,024 threads per block
  â€¢ 65,536 (64K) 32-bit registers per SM
```

> âš ï¸ **Both max blocks and warps per SM are independent hardware limits. Whichever one you hit first is your "bottleneck."**

### ğŸ½ï¸ Restaurant Analogy

Imagine a restaurant (the SM) with:
- ğŸª‘ **Total Seats**: 2,048 (Max Threads)
- ğŸ·ï¸ **Total Tables**: 32 (Max Blocks)

#### Scenario A: Huge Groups (1,024 threads per block)

```
Group 1: 1,024 seats used, 1 table used
Group 2: 2,048 seats used, 2 tables used
Result: âŒ Out of seats! (30 empty tables left)
```

#### Scenario B: Tiny Groups (32 threads per block)

```
32 groups Ã— 32 people = 1,024 people
Result: âŒ All 32 tables used, but 1,024 seats empty!
       (SM only has 32 "check-in" slots for block metadata)
```

### Calculating Full Occupancy

```
For full occupancy:
  65,536 registers / 2,048 threads = 32 registers per thread
```

> âœ… Each thread should use **no more than 32 registers** for full occupancy

---

## ğŸ” Querying Device Properties

### Getting Device Count

```c
int devCount;
cudaGetDeviceCount(&devCount);
```

### Iterating Through Devices

```c
cudaDeviceProp devProp;
for(unsigned int i = 0; i < devCount; i++) {
    cudaGetDeviceProperties(&devProp, i);
    // Decide if device has sufficient resources/capabilities
}
```

### ğŸ“‹ Important Properties

The `cudaDeviceProp` struct contains fields for device properties:

| Property | Description |
|----------|-------------|
| `devProp.maxThreadsPerBlock` | Maximum threads per block |
| `devProp.multiProcessorCount` | Number of SMs in the device |
| `devProp.clockRate` | Clock frequency of the device |
| `devProp.maxThreadsDim[0]` | Max threads in x dimension |
| `devProp.maxThreadsDim[1]` | Max threads in y dimension |
| `devProp.maxThreadsDim[2]` | Max threads in z dimension |

---